{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smmohua/INFO-5731-Fall-22/blob/main/INFO5731_Assignment_Three_2nd%20time_fall2022.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWxodXh5n4xF"
      },
      "source": [
        "# **Question 1: Understand N-gram**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TenBkDJ5n95k"
      },
      "source": [
        "(45 points). Write a python program to conduct N-gram analysis based on the dataset in your assignment two:\n",
        "\n",
        "(1) Count the frequency of all the N-grams (N=3).\n",
        "\n",
        "(2) Calculate the probabilities for all the bigrams in the dataset by using the fomular count(w2 w1) / count(w2). For example, count(really like) / count(really) = 1 / 3 = 0.33.\n",
        "\n",
        "(3) Extract all the **noun phrases** and calculate the relative probabilities of each review in terms of other reviews (abstracts, or tweets) by using the fomular frequency (noun phrase) / max frequency (noun phrase) on the whole dataset. Print out the result in a table with column name the all the noun phrases and row name as all the 100 reviews (abstracts, or tweets). \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PuFPKhC0m1fd",
        "outputId": "0793eede-590d-4fd6-f1fe-497a4fc8d34e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "\r0% [Connecting to archive.ubuntu.com (185.125.190.39)] [Connecting to security.\r0% [Connecting to archive.ubuntu.com (185.125.190.39)] [Connecting to security.\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (185.125.190.39\r                                                                               \rIgn:2 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (185.125.190.39\r                                                                               \rHit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:5 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:7 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:9 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:11 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [83.3 kB]\n",
            "Hit:13 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Fetched 264 kB in 3s (85.7 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-codecs-ffmpeg-extra\n",
            "Suggested packages:\n",
            "  webaccounts-chromium-extension unity-chromium-extension\n",
            "The following NEW packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-chromedriver\n",
            "  chromium-codecs-ffmpeg-extra\n",
            "0 upgraded, 4 newly installed, 0 to remove and 20 not upgraded.\n",
            "Need to get 95.6 MB of archives.\n",
            "After this operation, 321 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-codecs-ffmpeg-extra amd64 108.0.5359.71-0ubuntu0.18.04.5 [1,159 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser amd64 108.0.5359.71-0ubuntu0.18.04.5 [83.6 MB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser-l10n all 108.0.5359.71-0ubuntu0.18.04.5 [5,230 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-chromedriver amd64 108.0.5359.71-0ubuntu0.18.04.5 [5,594 kB]\n",
            "Fetched 95.6 MB in 7s (14.5 MB/s)\n",
            "Selecting previously unselected package chromium-codecs-ffmpeg-extra.\n",
            "(Reading database ... 124016 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-codecs-ffmpeg-extra_108.0.5359.71-0ubuntu0.18.04.5_amd64.deb ...\n",
            "Unpacking chromium-codecs-ffmpeg-extra (108.0.5359.71-0ubuntu0.18.04.5) ...\n",
            "Selecting previously unselected package chromium-browser.\n",
            "Preparing to unpack .../chromium-browser_108.0.5359.71-0ubuntu0.18.04.5_amd64.deb ...\n",
            "Unpacking chromium-browser (108.0.5359.71-0ubuntu0.18.04.5) ...\n",
            "Selecting previously unselected package chromium-browser-l10n.\n",
            "Preparing to unpack .../chromium-browser-l10n_108.0.5359.71-0ubuntu0.18.04.5_all.deb ...\n",
            "Unpacking chromium-browser-l10n (108.0.5359.71-0ubuntu0.18.04.5) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_108.0.5359.71-0ubuntu0.18.04.5_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (108.0.5359.71-0ubuntu0.18.04.5) ...\n",
            "Setting up chromium-codecs-ffmpeg-extra (108.0.5359.71-0ubuntu0.18.04.5) ...\n",
            "Setting up chromium-browser (108.0.5359.71-0ubuntu0.18.04.5) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (108.0.5359.71-0ubuntu0.18.04.5) ...\n",
            "Setting up chromium-browser-l10n (108.0.5359.71-0ubuntu0.18.04.5) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.6) ...\n",
            "cp: '/usr/lib/chromium-browser/chromedriver' and '/usr/bin/chromedriver' are the same file\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting selenium\n",
            "  Downloading selenium-4.7.2-py3-none-any.whl (6.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.3 MB 6.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.8/dist-packages (from selenium) (2022.12.7)\n",
            "Collecting trio~=0.17\n",
            "  Downloading trio-0.22.0-py3-none-any.whl (384 kB)\n",
            "\u001b[K     |████████████████████████████████| 384 kB 60.4 MB/s \n",
            "\u001b[?25hCollecting trio-websocket~=0.9\n",
            "  Downloading trio_websocket-0.9.2-py3-none-any.whl (16 kB)\n",
            "Collecting urllib3[socks]~=1.26\n",
            "  Downloading urllib3-1.26.13-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 53.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sortedcontainers in /usr/local/lib/python3.8/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Collecting outcome\n",
            "  Downloading outcome-1.2.0-py2.py3-none-any.whl (9.7 kB)\n",
            "Collecting sniffio\n",
            "  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
            "Collecting exceptiongroup>=1.0.0rc9\n",
            "  Downloading exceptiongroup-1.0.4-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.8/dist-packages (from trio~=0.17->selenium) (2.10)\n",
            "Collecting async-generator>=1.9\n",
            "  Downloading async_generator-1.10-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.8/dist-packages (from trio~=0.17->selenium) (22.1.0)\n",
            "Collecting wsproto>=0.14\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.8/dist-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n",
            "Collecting h11<1,>=0.9.0\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 5.4 MB/s \n",
            "\u001b[?25hInstalling collected packages: sniffio, outcome, h11, exceptiongroup, async-generator, wsproto, urllib3, trio, trio-websocket, selenium\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "requests 2.23.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you have urllib3 1.26.13 which is incompatible.\u001b[0m\n",
            "Successfully installed async-generator-1.10 exceptiongroup-1.0.4 h11-0.14.0 outcome-1.2.0 selenium-4.7.2 sniffio-1.3.0 trio-0.22.0 trio-websocket-0.9.2 urllib3-1.26.13 wsproto-1.2.0\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "#installing selenium package\n",
        "!apt-get update\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "!pip install selenium\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver #importing selenium package\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('-headless')\n",
        "options.add_argument('-no-sandbox')\n",
        "options.add_argument('-disable-dev-shm-usage')"
      ],
      "metadata": {
        "id": "ueCCFakHR5_H"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium.webdriver.support.ui import WebDriverWait as wait\n",
        "from selenium.webdriver.common.by import By\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "driver = webdriver.Chrome('chromedriver',options=options)\n",
        "link = 'https://www.imdb.com/title/tt7286456/reviews?ref_=tt_urv'\n",
        "title_array = []\n",
        "review_array = []\n",
        "driver.get(link)\n",
        "for num in range(4):\n",
        "  driver.find_element(By.CLASS_NAME,\"ipl-load-more__button\").click()\n",
        "  time.sleep(5)\n",
        "  listOfTitle = driver.find_elements(By.CLASS_NAME, \"title\")\n",
        "  listOfReviews = driver.find_elements(By.CLASS_NAME, \"text\")\n",
        "for ele, sub_ele in zip(listOfTitle, listOfReviews):\n",
        "      title_array.append((ele.text).replace('\\n',''))\n",
        "      review_array.append(sub_ele.text)\n",
        "df = pd.DataFrame(list(zip(title_array, review_array)), columns =['Title', 'Review'])\n",
        "print(\"Length of data frame is {0}\".format(len(df)))\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "MauBE0ahPA-P",
        "outputId": "9e2da2d0-0f0a-4edc-b39d-a3c48c57823f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of data frame is 125\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                 Title  \\\n",
              "0        I would not call it a masterpiece as some did   \n",
              "1        Not a spoon feeding of CGI fueled faux drama.   \n",
              "2                           Is it great? I don't know.   \n",
              "3                             Perfect in every aspect.   \n",
              "4    A psychological study, rather than a superhero...   \n",
              "..                                                 ...   \n",
              "120                            There Are No Capes Here   \n",
              "121  If you're an outcast, you'll understand this m...   \n",
              "122                   An experience as much as a movie   \n",
              "123  Incredible movie! And one of the best 2019 mov...   \n",
              "124                                       Scarily real   \n",
              "\n",
              "                                                Review  \n",
              "0                                                       \n",
              "1    The movie affects you in a way that makes it p...  \n",
              "2    When I heard everyone saying that this is the ...  \n",
              "3    Truly a masterpiece, The Best Hollywood film o...  \n",
              "4    I have seen Joker yesterday at Venice an early...  \n",
              "..                                                 ...  \n",
              "120  Joaquin Phoenix transforms into the titular ch...  \n",
              "121  And I'm not saying you need to be a freak. Man...  \n",
              "122  Saw this at the cinema and came out speechless...  \n",
              "123  This is the movie where the audience will like...  \n",
              "124  First just to say that I would officially rate...  \n",
              "\n",
              "[125 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2cc00fe8-1748-4ecc-9429-5ee604898481\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>Review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I would not call it a masterpiece as some did</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Not a spoon feeding of CGI fueled faux drama.</td>\n",
              "      <td>The movie affects you in a way that makes it p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Is it great? I don't know.</td>\n",
              "      <td>When I heard everyone saying that this is the ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Perfect in every aspect.</td>\n",
              "      <td>Truly a masterpiece, The Best Hollywood film o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>A psychological study, rather than a superhero...</td>\n",
              "      <td>I have seen Joker yesterday at Venice an early...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120</th>\n",
              "      <td>There Are No Capes Here</td>\n",
              "      <td>Joaquin Phoenix transforms into the titular ch...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>121</th>\n",
              "      <td>If you're an outcast, you'll understand this m...</td>\n",
              "      <td>And I'm not saying you need to be a freak. Man...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>122</th>\n",
              "      <td>An experience as much as a movie</td>\n",
              "      <td>Saw this at the cinema and came out speechless...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123</th>\n",
              "      <td>Incredible movie! And one of the best 2019 mov...</td>\n",
              "      <td>This is the movie where the audience will like...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>124</th>\n",
              "      <td>Scarily real</td>\n",
              "      <td>First just to say that I would officially rate...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>125 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2cc00fe8-1748-4ecc-9429-5ee604898481')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2cc00fe8-1748-4ecc-9429-5ee604898481 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2cc00fe8-1748-4ecc-9429-5ee604898481');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhYIYaqsaepb",
        "outputId": "8df0e5bc-a7bf-4385-9b35-700dbb262957"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USSdXHuqnwv9"
      },
      "source": [
        "# **INFO5731 Assignment Three**\n",
        "\n",
        "In this assignment, you are required to conduct information extraction, semantic analysis based on **the dataset you collected from assignment two**. You may use scipy and numpy package in this assignment."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['Lower Case'] = df['Review'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
        "df['Removal of Punctuation'] = df['Lower Case'].str.replace('[^\\w\\s]','')"
      ],
      "metadata": {
        "id": "VMg59X5waes_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "df['Removal of Special Characters'] = df['Removal of Punctuation'].apply(lambda x: ''.join(re.sub(r\"[^a-zA-Z0-9]+\", ' ', charctr) for charctr in x ))\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop = stopwords.words('english')\n",
        "df['Stopwords Removal'] = df['Removal of Punctuation'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))"
      ],
      "metadata": {
        "id": "vnTFGN83aewk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "df['Spelling Correction'] = df['Stopwords Removal'].apply(lambda x: str(TextBlob(x).correct()))\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "df['Tokenization'] = df['Spelling Correction'].apply(lambda x: TextBlob(x).words)"
      ],
      "metadata": {
        "id": "jbJSDQ-oae2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "st = PorterStemmer()\n",
        "df['After Stemming'] = df['Tokenization'].apply(lambda x: \" \".join([st.stem(word) for word in x]))\n",
        "from textblob import Word\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "df['After Lemmatization'] = df['After Stemming'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
        "df.to_csv('/content/sample_data/reviwes.csv',index=False)"
      ],
      "metadata": {
        "id": "9JboEEAIae6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import collections\n",
        "from nltk import ngrams\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "words_sentence = []\n",
        "for sentence in df['After Lemmatization']:\n",
        "  words_sentence.append(word_tokenize(sentence))\n",
        "aftercleaning = [x for x in words_sentence if x != []]\n",
        "iterations = list(itertools.chain.from_iterable(aftercleaning))"
      ],
      "metadata": {
        "id": "mOtZz99hae9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tri_grams = nltk.trigrams(iterations)\n",
        "frequency_dist = nltk.FreqDist(tri_grams)\n",
        "frequency_dist"
      ],
      "metadata": {
        "id": "m7HyYm7kafBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "bi_grams = nltk.bigrams(iterations)\n",
        "frequency_dist = nltk.FreqDist(bi_grams)\n",
        "bi_grams_dict = dict(frequency_dist)\n",
        "for word in bi_grams_dict:\n",
        "  print( str(word) + ':' + str(bi_grams_dict[word] / iterations.count(word[0])))\n"
      ],
      "metadata": {
        "id": "k-gb4jz_afEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "whole_dataset = ''\n",
        "index = []\n",
        "i = 1\n",
        "for line in df['After Lemmatization']:\n",
        "  whole_dataset = whole_dataset + line\n",
        "  value = 'Review-' + str(i)\n",
        "  index.append(value)\n",
        "  i += 1"
      ],
      "metadata": {
        "id": "YB3-p_FOafHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('brown')\n",
        "from textblob import TextBlob\n",
        "noun_phrases = []\n",
        "frequency = []\n",
        "for line in df['After Lemmatization']:\n",
        "  blob = TextBlob(line)\n",
        "  for nouns in blob.noun_phrases:\n",
        "    noun_phrases.append(nouns)\n",
        "for word in noun_phrases:\n",
        "  noun_phrases_freq = []\n",
        "  for line in df['After Lemmatization']:\n",
        "    noun_phrases_freq.append(line.count(word) / whole_dataset.count(word))\n",
        "  frequency.append(noun_phrases_freq)\n",
        "noun_phrases_df = pd.DataFrame(frequency).T\n",
        "noun_phrases_df.columns = list(noun_phrases)\n",
        "noun_phrases_df.index = index\n",
        "noun_phrases_df  "
      ],
      "metadata": {
        "id": "oT2H6yFWafKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfpMRCrRwN6Z"
      },
      "source": [
        "# **Question 2: Undersand TF-IDF and Document representation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dCQEbDawWCw"
      },
      "source": [
        "(20 points). Starting from the documents (all the reviews, or abstracts, or tweets) collected for assignment two, write a python program: \n",
        "\n",
        "(1) To build the **documents-terms weights (tf*idf) matrix bold text**.\n",
        "\n",
        "(2) To rank the documents with respect to query (design a query by yourself, for example, \"An Outstanding movie with a haunting performance and best character development\") by using **cosine similarity**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vATjQNTY8buA"
      },
      "outputs": [],
      "source": [
        "# Write your code here\n",
        "import math\n",
        "def tf_idf_value(sentence,word):\n",
        "  size = len(sentence.split(\" \"))\n",
        "  tf_value = sentence.count(word)/size\n",
        "  idf_value = 0\n",
        "  if(tf_value!=0):\n",
        "    idf_value = math.log(size)/sentence.count(word)\n",
        "  else:\n",
        "    return 0;\n",
        "  return tf_value*idf_value\n",
        "\n",
        "sentences = df[\"Spelling Correction\"].values.tolist()\n",
        "tokens_list = set([j for i in sentences for j in i.split(\" \")])\n",
        "tf_idf = pd.DataFrame(tokens_list,columns=[\"token\"])\n",
        "count = 0\n",
        "for i in sentences:\n",
        "  tf_idf[str(count)] = tf_idf[\"token\"].apply(lambda x : tf_idf_value(i,x))\n",
        "  count+=1\n",
        "\n",
        "tf_idf\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "T4X2o8UTdLVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "dtIHG2PXdLYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_sentence = 'Shang-Chi and the Legend of the Ten Rings is an Action - Fantasy movie in which we watch a new era of Marvel Studios rise with Shang-Chi,the master of Kung Fu. He has to face his past involving him with the Ten Rings organization and crate his future accordingly.Finally, I have to say that \"Shang-Chi and the Legend of the Ten Rings\" is an amazing movie and I strongly recommend everyone to watch it, especially the fans of Marvel.'\n",
        "X_list = word_tokenize(query_sentence)\n",
        "sw = stopwords.words('english') \n",
        "X_set = {w for w in X_list if not w in sw}"
      ],
      "metadata": {
        "id": "Im0qa3QVdWgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_vector(X_set, Y_set):\n",
        "  rvector = X_set.union(Y_set)\n",
        "  l1 =[];l2 =[]\n",
        "  for w in rvector: \n",
        "      if w in X_set: l1.append(1)\n",
        "      else: l1.append(0) \n",
        "      if w in Y_set: l2.append(1) \n",
        "      else: l2.append(0)\n",
        "  return rvector, l1, l2"
      ],
      "metadata": {
        "id": "YTQLOwHKdWkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cosine(rvector, l1, l2):\n",
        "  c = 0\n",
        "  for i in range(len(rvector)): \n",
        "        c+= l1[i]*l2[i] \n",
        "  cosine = c / float((sum(l1)*sum(l2))**0.5) \n",
        "  return cosine"
      ],
      "metadata": {
        "id": "Gxv4HCFbdgUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "similarity_list = []\n",
        "for sentence in df['Spelling Correction']:\n",
        "  Y_list = word_tokenize(sentence)\n",
        "  Y_set = {w for w in Y_list if not w in sw}\n",
        "  rvector, l1, l2 = compute_vector(X_set, Y_set)\n",
        "  try:\n",
        "    similarity = compute_cosine(rvector, l1, l2)\n",
        "  except ZeroDivisionError:\n",
        "    similarity = 'None'\n",
        "  similarity_list.append(similarity)\n",
        "cosine_dataframe = pd.DataFrame(list(zip(df['Review'],similarity_list)), columns=['Reviews','Cosine Similarity'])\n",
        "cosine_dataframe"
      ],
      "metadata": {
        "id": "eJD9VIWHdgZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diTYPzILO0Cj"
      },
      "source": [
        "# **Question 3: Create your own word embedding model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1Ht0whXO0Ck"
      },
      "source": [
        "(20 points). Use the data you collected for assignment two to build a word embedding model: \n",
        "\n",
        "(1) Train a 300-dimension word embedding (it can be word2vec, glove, ulmfit, bert, or others).\n",
        "\n",
        "(2) Visualize the word embedding model you created.\n",
        "\n",
        "Reference: https://machinelearningmastery.com/develop-word-embeddings-python-gensim/\n",
        "\n",
        "Reference: https://jaketae.github.io/study/word2vec/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cV1F09kCO0Cl"
      },
      "outputs": [],
      "source": [
        "# Write your code here\n",
        "!apt-get update\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "!pip install selenium\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver #importing selenium package\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('-headless')\n",
        "options.add_argument('-no-sandbox')\n",
        "options.add_argument('-disable-dev-shm-usage')"
      ],
      "metadata": {
        "id": "rB5L-LmPd1Hu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium.webdriver.support.ui import WebDriverWait as wait\n",
        "from selenium.webdriver.common.by import By\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "driver = webdriver.Chrome('chromedriver',options=options)\n",
        "link = 'https://www.imdb.com/title/tt7286456/reviews?ref_=tt_urv'\n",
        "title_array = []\n",
        "review_array = []\n",
        "driver.get(link)\n",
        "for num in range(4):\n",
        "  driver.find_element(By.CLASS_NAME,\"ipl-load-more__button\").click()\n",
        "  time.sleep(5)\n",
        "  listOfTitle = driver.find_elements(By.CLASS_NAME, \"title\")\n",
        "  listOfReviews = driver.find_elements(By.CLASS_NAME, \"text\")\n",
        "for ele, sub_ele in zip(listOfTitle, listOfReviews):\n",
        "      title_array.append((ele.text).replace('\\n',''))\n",
        "      review_array.append(sub_ele.text)\n",
        "df = pd.DataFrame(list(zip(title_array, review_array)), columns =['Title', 'Review'])\n",
        "print(\"Length of data frame is {0}\".format(len(df)))\n",
        "df"
      ],
      "metadata": {
        "id": "xfZXCbAUd1L1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list2 = []\n",
        "for i in df.index:\n",
        "    y = str(df[\"Review\"][i]).split()\n",
        "    list2.append(y)"
      ],
      "metadata": {
        "id": "8bXL3rEHd1Pf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "model2 = Word2Vec(list2, size = 300)\n",
        "print(model2)"
      ],
      "metadata": {
        "id": "_xBIJMOXd1TT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words2 = list(model2.wv.vocab)\n",
        "words2"
      ],
      "metadata": {
        "id": "V_CTzPVld1V7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fit a 2d PCA model to the vectors\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "X = model2[model2.wv.vocab]\n",
        "pca = PCA(n_components=2)\n",
        "result = pca.fit_transform(X)"
      ],
      "metadata": {
        "id": "WcJ81_O6eezD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a scatter plot of the projection\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure (figsize = (20,20))\n",
        "plt.scatter(result[:, 0], result[:, 1])\n",
        "words = list(model2.wv.vocab)\n",
        "for i, word in enumerate(words):\n",
        "\tplt.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sBXhWhJ6ee2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5mmYIfN8eYV"
      },
      "source": [
        "# **Question 4: Create your own training and evaluation data for sentiment analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsi2y4z88ngX"
      },
      "source": [
        "(15 points). **You dodn't need to write program for this question!** Read each review (abstract or tweet) you collected in detail, and annotate each review with a sentiment (positive, negative, or neutral). Save the annotated dataset into a csv file with three columns (first column: document_id, clean_text, sentiment), upload the csv file to GitHub and submit the file link blew. This datset will be used for assignment four: sentiment analysis and text classification. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XfvMKJjIXS5G"
      },
      "outputs": [],
      "source": [
        "# The GitHub link of your final csv file\n",
        "\n",
        "\n",
        "\n",
        "# Link: https://github.com/smmohua/INFO-5731-Fall-22/blob/01bd400e21989e5bbee2ca3437706e920726980a/xyz\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
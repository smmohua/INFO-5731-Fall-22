{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **The fourth in-class-exercise (40 points in total, 11/03/2022)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question description: Please use the text corpus you collected in your last in-class-exercise for this exercise. Perform the following tasks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) (10 points) Generate K topics by using LDA, the number of topics K should be decided by the coherence score, then summarize what are the topics. You may refer the code here: \n",
    "\n",
    "https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Review</th>\n",
       "      <th>Reviews after Noise Removal</th>\n",
       "      <th>After digits removal</th>\n",
       "      <th>Stopwords Removal</th>\n",
       "      <th>Lower Case</th>\n",
       "      <th>After Stemming</th>\n",
       "      <th>After Lemmatization</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Perfect in every aspect.</td>\n",
       "      <td>Truly a masterpiece, The Best Hollywood film o...</td>\n",
       "      <td>Truly a masterpiece The Best Hollywood film of...</td>\n",
       "      <td>Truly a masterpiece The Best Hollywood film of...</td>\n",
       "      <td>Truly masterpiece The Best Hollywood film one ...</td>\n",
       "      <td>truly masterpiece the best hollywood film one ...</td>\n",
       "      <td>t r u l y   m a s t e r p i e c e   t h e   b ...</td>\n",
       "      <td>t r u l y m a s t e r p i e c e t h e b e s t ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A psychological study, rather than a superhero...</td>\n",
       "      <td>I have seen Joker yesterday at Venice an early...</td>\n",
       "      <td>I have seen Joker yesterday at Venice an early...</td>\n",
       "      <td>I have seen Joker yesterday at Venice an early...</td>\n",
       "      <td>I seen Joker yesterday Venice early illfated s...</td>\n",
       "      <td>i seen joker yesterday venice early illfated s...</td>\n",
       "      <td>i   s e e n   j o k e r   y e s t e r d a y   ...</td>\n",
       "      <td>i s e e n j o k e r y e s t e r d a y v e n i ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Went for a second time to watch</td>\n",
       "      <td>I get why some people hate this . It's because...</td>\n",
       "      <td>I get why some people hate this  Its because o...</td>\n",
       "      <td>I get why some people hate this  Its because o...</td>\n",
       "      <td>I get people hate Its political message people...</td>\n",
       "      <td>i get people hate its political message people...</td>\n",
       "      <td>i   g e t   p e o p l e   h a t e   i t s   p ...</td>\n",
       "      <td>i g e t p e o p l e h a t e i t s p o l i t i ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JUST AMAZING. How does this movie exist.</td>\n",
       "      <td>Let me start off by saying if Joaquin Phoneix ...</td>\n",
       "      <td>Let me start off by saying if Joaquin Phoneix ...</td>\n",
       "      <td>Let me start off by saying if Joaquin Phoneix ...</td>\n",
       "      <td>Let start saying Joaquin Phoneix doesnt get Os...</td>\n",
       "      <td>let start saying joaquin phoneix doesnt get os...</td>\n",
       "      <td>l e t   s t a r t   s a y i n g   j o a q u i ...</td>\n",
       "      <td>l e t s t a r t s a y i n g j o a q u i n p h ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Outstanding movie with a haunting performance ...</td>\n",
       "      <td>Every once in a while a movie comes, that trul...</td>\n",
       "      <td>Every once in a while a movie comes that truly...</td>\n",
       "      <td>Every once in a while a movie comes that truly...</td>\n",
       "      <td>Every movie comes truly makes impact Joaquins ...</td>\n",
       "      <td>every movie comes truly makes impact joaquins ...</td>\n",
       "      <td>e v e r y   m o v i e   c o m e s   t r u l y ...</td>\n",
       "      <td>e v e r y m o v i e c o m e s t r u l y m a k ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>I don't get what everyone else seems to be see...</td>\n",
       "      <td>I do agree that Joaquin's performance was grea...</td>\n",
       "      <td>I do agree that Joaquins performance was great...</td>\n",
       "      <td>I do agree that Joaquins performance was great...</td>\n",
       "      <td>I agree Joaquins performance great well done e...</td>\n",
       "      <td>i agree joaquins performance great well done e...</td>\n",
       "      <td>i   a g r e e   j o a q u i n s   p e r f o r ...</td>\n",
       "      <td>i a g r e e j o a q u i n s p e r f o r m a n ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>Led by a stunning Joaquin Phoenix, Joker is an...</td>\n",
       "      <td>Whether you end up loving or hating Joker, Tod...</td>\n",
       "      <td>Whether you end up loving or hating Joker Todd...</td>\n",
       "      <td>Whether you end up loving or hating Joker Todd...</td>\n",
       "      <td>Whether end loving hating Joker Todd Phillips ...</td>\n",
       "      <td>whether end loving hating joker todd phillips ...</td>\n",
       "      <td>w h e t h e r   e n d   l o v i n g   h a t i ...</td>\n",
       "      <td>w h e t h e r e n d l o v i n g h a t i n g j ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>Hats off to Joaquin phoenix</td>\n",
       "      <td>What a legendary character and what a legendar...</td>\n",
       "      <td>What a legendary character and what a legendar...</td>\n",
       "      <td>What a legendary character and what a legendar...</td>\n",
       "      <td>What legendary character legendary performance...</td>\n",
       "      <td>what legendary character legendary performance...</td>\n",
       "      <td>w h a t   l e g e n d a r y   c h a r a c t e ...</td>\n",
       "      <td>w h a t l e g e n d a r y c h a r a c t e r l ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>Not at all deserving of the hype</td>\n",
       "      <td>It was well played film. Of course JF does his...</td>\n",
       "      <td>It was well played film Of course JF does his ...</td>\n",
       "      <td>It was well played film Of course JF does his ...</td>\n",
       "      <td>It well played film Of course JF due diligence...</td>\n",
       "      <td>it well played film of course jf due diligence...</td>\n",
       "      <td>i t   w e l l   p l a y e d   f i l m   o f   ...</td>\n",
       "      <td>i t w e l l p l a y e d f i l m o f c o u r s ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>The Joker is supposed to be a diabolical maste...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>124 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Title  \\\n",
       "0                             Perfect in every aspect.   \n",
       "1    A psychological study, rather than a superhero...   \n",
       "2                      Went for a second time to watch   \n",
       "3             JUST AMAZING. How does this movie exist.   \n",
       "4    Outstanding movie with a haunting performance ...   \n",
       "..                                                 ...   \n",
       "119  I don't get what everyone else seems to be see...   \n",
       "120  Led by a stunning Joaquin Phoenix, Joker is an...   \n",
       "121                        Hats off to Joaquin phoenix   \n",
       "122                   Not at all deserving of the hype   \n",
       "123  The Joker is supposed to be a diabolical maste...   \n",
       "\n",
       "                                                Review  \\\n",
       "0    Truly a masterpiece, The Best Hollywood film o...   \n",
       "1    I have seen Joker yesterday at Venice an early...   \n",
       "2    I get why some people hate this . It's because...   \n",
       "3    Let me start off by saying if Joaquin Phoneix ...   \n",
       "4    Every once in a while a movie comes, that trul...   \n",
       "..                                                 ...   \n",
       "119  I do agree that Joaquin's performance was grea...   \n",
       "120  Whether you end up loving or hating Joker, Tod...   \n",
       "121  What a legendary character and what a legendar...   \n",
       "122  It was well played film. Of course JF does his...   \n",
       "123                                                NaN   \n",
       "\n",
       "                           Reviews after Noise Removal  \\\n",
       "0    Truly a masterpiece The Best Hollywood film of...   \n",
       "1    I have seen Joker yesterday at Venice an early...   \n",
       "2    I get why some people hate this  Its because o...   \n",
       "3    Let me start off by saying if Joaquin Phoneix ...   \n",
       "4    Every once in a while a movie comes that truly...   \n",
       "..                                                 ...   \n",
       "119  I do agree that Joaquins performance was great...   \n",
       "120  Whether you end up loving or hating Joker Todd...   \n",
       "121  What a legendary character and what a legendar...   \n",
       "122  It was well played film Of course JF does his ...   \n",
       "123                                                NaN   \n",
       "\n",
       "                                  After digits removal  \\\n",
       "0    Truly a masterpiece The Best Hollywood film of...   \n",
       "1    I have seen Joker yesterday at Venice an early...   \n",
       "2    I get why some people hate this  Its because o...   \n",
       "3    Let me start off by saying if Joaquin Phoneix ...   \n",
       "4    Every once in a while a movie comes that truly...   \n",
       "..                                                 ...   \n",
       "119  I do agree that Joaquins performance was great...   \n",
       "120  Whether you end up loving or hating Joker Todd...   \n",
       "121  What a legendary character and what a legendar...   \n",
       "122  It was well played film Of course JF does his ...   \n",
       "123                                                NaN   \n",
       "\n",
       "                                     Stopwords Removal  \\\n",
       "0    Truly masterpiece The Best Hollywood film one ...   \n",
       "1    I seen Joker yesterday Venice early illfated s...   \n",
       "2    I get people hate Its political message people...   \n",
       "3    Let start saying Joaquin Phoneix doesnt get Os...   \n",
       "4    Every movie comes truly makes impact Joaquins ...   \n",
       "..                                                 ...   \n",
       "119  I agree Joaquins performance great well done e...   \n",
       "120  Whether end loving hating Joker Todd Phillips ...   \n",
       "121  What legendary character legendary performance...   \n",
       "122  It well played film Of course JF due diligence...   \n",
       "123                                                NaN   \n",
       "\n",
       "                                            Lower Case  \\\n",
       "0    truly masterpiece the best hollywood film one ...   \n",
       "1    i seen joker yesterday venice early illfated s...   \n",
       "2    i get people hate its political message people...   \n",
       "3    let start saying joaquin phoneix doesnt get os...   \n",
       "4    every movie comes truly makes impact joaquins ...   \n",
       "..                                                 ...   \n",
       "119  i agree joaquins performance great well done e...   \n",
       "120  whether end loving hating joker todd phillips ...   \n",
       "121  what legendary character legendary performance...   \n",
       "122  it well played film of course jf due diligence...   \n",
       "123                                                NaN   \n",
       "\n",
       "                                        After Stemming  \\\n",
       "0    t r u l y   m a s t e r p i e c e   t h e   b ...   \n",
       "1    i   s e e n   j o k e r   y e s t e r d a y   ...   \n",
       "2    i   g e t   p e o p l e   h a t e   i t s   p ...   \n",
       "3    l e t   s t a r t   s a y i n g   j o a q u i ...   \n",
       "4    e v e r y   m o v i e   c o m e s   t r u l y ...   \n",
       "..                                                 ...   \n",
       "119  i   a g r e e   j o a q u i n s   p e r f o r ...   \n",
       "120  w h e t h e r   e n d   l o v i n g   h a t i ...   \n",
       "121  w h a t   l e g e n d a r y   c h a r a c t e ...   \n",
       "122  i t   w e l l   p l a y e d   f i l m   o f   ...   \n",
       "123                                                NaN   \n",
       "\n",
       "                                   After Lemmatization  \n",
       "0    t r u l y m a s t e r p i e c e t h e b e s t ...  \n",
       "1    i s e e n j o k e r y e s t e r d a y v e n i ...  \n",
       "2    i g e t p e o p l e h a t e i t s p o l i t i ...  \n",
       "3    l e t s t a r t s a y i n g j o a q u i n p h ...  \n",
       "4    e v e r y m o v i e c o m e s t r u l y m a k ...  \n",
       "..                                                 ...  \n",
       "119  i a g r e e j o a q u i n s p e r f o r m a n ...  \n",
       "120  w h e t h e r e n d l o v i n g h a t i n g j ...  \n",
       "121  w h a t l e g e n d a r y c h a r a c t e r l ...  \n",
       "122  i t w e l l p l a y e d f i l m o f c o u r s ...  \n",
       "123                                                NaN  \n",
       "\n",
       "[124 rows x 8 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write your code here\n",
    "\n",
    "#import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "import argparse\n",
    "\n",
    "df = pd.read_csv('./joker_reviews.csv')\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "stunning the loved lazy worthless off it hats believe for\n",
      "Topic 1:\n",
      "astonishing joking masterpiece can joker study rather laughter people matter\n",
      "Topic 2:\n",
      "to crap movie what wrong plot make context how in\n",
      "Topic 3:\n",
      "overrated and an extremely ever mirror film ok boring idiot\n",
      "Topic 4:\n",
      "movie how to considerably exist about know okay does something\n",
      "Topic 5:\n",
      "joke the is joker like this abusive joaquin it movie\n",
      "Topic 6:\n",
      "not tedious that dark good but the and just depressing\n",
      "Topic 7:\n",
      "it joking don comedy endgame perfect get man uprising phoenix\n",
      "Topic 8:\n",
      "masterpiece clown overhyped not brilliant funny depressive useless character work\n",
      "Topic 9:\n",
      "of bad real if relentlessly movie kind murdering incel lurid\n",
      "Document 0: about all 76 10 abusive account affect alone acting\n",
      "Document 1: 76 affect 10 abusive about am all acting account\n",
      "Document 2: about 10 alone acting all account affect am 76\n",
      "Document 3: account about acting am affect 76 10 abusive all\n",
      "Document 4: abusive account affect am acting about all alone 76\n",
      "Document 5: 76 affect abusive 10 account about am alone acting\n",
      "Document 6: alone affect all acting 10 about abusive 76 am\n",
      "Document 7: affect acting am all alone 76 10 account abusive\n",
      "Document 8: all 10 account acting alone about affect 76 abusive\n",
      "Document 9: alone 76 10 all about am acting abusive affect\n",
      "Document 10: alone affect am acting all account abusive 76 10\n",
      "Document 11: all acting 76 am abusive account 10 affect alone\n",
      "Document 12: acting am 10 all alone abusive affect account 76\n",
      "Document 13: all acting alone account about am abusive 10 76\n",
      "Document 14: acting account about 10 abusive affect alone am all\n",
      "Document 15: abusive acting account am affect about 76 10 all\n",
      "Document 16: affect alone about 76 acting am 10 abusive account\n",
      "Document 17: affect acting about am 10 account all abusive 76\n",
      "Document 18: affect acting all am abusive alone 76 account about\n",
      "Document 19: affect 10 abusive all acting about alone 76 am\n",
      "Document 20: acting affect abusive alone am 76 10 about all\n",
      "Document 21: 10 all acting about abusive am affect alone 76\n",
      "Document 22: 10 abusive account about am alone all 76 affect\n",
      "Document 23: all alone am acting 10 affect about abusive 76\n",
      "Document 24: abusive about affect alone am acting 76 account 10\n",
      "Document 25: abusive alone account acting affect 10 about 76 am\n",
      "Document 26: all about am abusive acting affect alone 76 account\n",
      "Document 27: abusive acting alone affect all 76 am account about\n",
      "Document 28: affect all account about am abusive alone 76 acting\n",
      "Document 29: acting affect alone abusive 10 am all about account\n",
      "Document 30: affect about acting abusive all account 10 76 am\n",
      "Document 31: am alone all affect acting 10 account abusive 76\n",
      "Document 32: all 76 alone acting am affect about abusive account\n",
      "Document 33: about affect am 76 account abusive 10 all alone\n",
      "Document 34: acting all alone 10 am about abusive 76 account\n",
      "Document 35: about acting account all am 10 affect alone abusive\n",
      "Document 36: am abusive about 76 affect all alone acting account\n",
      "Document 37: all about am 10 acting account abusive alone affect\n",
      "Document 38: alone affect abusive am all acting about 76 account\n",
      "Document 39: all 10 acting affect am account alone about abusive\n",
      "Document 40: all 10 am acting about abusive affect account alone\n",
      "Document 41: alone all abusive about account am affect acting 10\n",
      "Document 42: acting all 76 alone 10 account about am affect\n",
      "Document 43: all abusive affect acting account am 76 about alone\n",
      "Document 44: am about account acting abusive alone 10 76 affect\n",
      "Document 45: affect 76 acting am about account alone 10 abusive\n",
      "Document 46: am affect abusive acting account 76 all alone 10\n",
      "Document 47: 10 all acting affect am alone abusive account 76\n",
      "Document 48: acting am 10 affect account abusive 76 all alone\n",
      "Document 49: account am all acting affect about 76 10 alone\n",
      "Document 50: 76 acting all alone abusive am account about 10\n",
      "Document 51: abusive alone am acting 76 about 10 account affect\n",
      "Document 52: acting am about alone all account abusive 10 76\n",
      "Document 53: acting am about account alone all affect abusive 10\n",
      "Document 54: abusive acting affect am 10 all account alone 76\n",
      "Document 55: alone 76 all account about affect am abusive 10\n",
      "Document 56: abusive about account alone all 10 acting affect 76\n",
      "Document 57: affect abusive alone all 76 acting account am about\n",
      "Document 58: am account abusive alone about affect all 10 acting\n",
      "Document 59: affect abusive alone am acting 76 all about account\n",
      "Document 60: all acting affect abusive alone 10 about 76 am\n",
      "Document 61: abusive all 10 affect alone about account 76 acting\n",
      "Document 62: acting about account am all abusive alone 76 10\n",
      "Document 63: all affect alone 76 acting abusive account about am\n",
      "Document 64: abusive alone affect 76 10 about all account am\n",
      "Document 65: 10 alone all acting 76 abusive am affect about\n",
      "Document 66: am affect acting account abusive about all 10 alone\n",
      "Document 67: affect all acting 10 about abusive alone am account\n",
      "Document 68: abusive alone account acting affect 10 about 76 am\n",
      "Document 69: abusive affect account acting alone 76 am all about\n",
      "Document 70: acting all affect 10 alone account 76 about am\n",
      "Document 71: am acting 10 all affect account abusive 76 alone\n",
      "Document 72: about acting 76 alone am 10 account affect abusive\n",
      "Document 73: abusive 76 10 about affect alone am account all\n",
      "Document 74: 10 acting alone 76 account am all affect abusive\n",
      "Document 75: acting am affect all 10 alone abusive about 76\n",
      "Document 76: about abusive all affect 10 76 alone am account\n",
      "Document 77: acting 76 10 am about account alone affect abusive\n",
      "Document 78: am affect alone abusive account 76 all 10 acting\n",
      "Document 79: alone acting abusive about account affect 76 10 all\n",
      "Document 80: acting alone 76 account all 10 affect about abusive\n",
      "Document 81: all about 76 account abusive 10 alone affect acting\n",
      "Document 82: 10 acting 76 am about abusive all affect alone\n",
      "Document 83: all 76 am account about 10 affect acting abusive\n",
      "Document 84: about account affect 10 all abusive alone 76 acting\n",
      "Document 85: 10 acting all am affect about account alone abusive\n",
      "Document 86: acting am abusive 10 all account 76 affect alone\n",
      "Document 87: 76 affect acting account all about 10 am alone\n",
      "Document 88: acting all 10 alone affect account abusive am 76\n",
      "Document 89: about 10 alone abusive all account am 76 acting\n",
      "Document 90: acting abusive am all 76 account 10 alone about\n",
      "Document 91: all abusive affect acting about alone account 76 am\n",
      "Document 92: acting account 10 am affect all abusive alone 76\n",
      "Document 93: 10 acting about 76 all affect account alone am\n",
      "Document 94: alone 10 affect about account am abusive acting 76\n",
      "Document 95: all affect alone am 76 about 10 account acting\n",
      "Document 96: abusive alone affect 76 10 about all account am\n",
      "Document 97: am abusive affect acting account about all alone 10\n",
      "Document 98: acting affect am abusive 76 10 about alone account\n",
      "Document 99: about abusive account affect am alone 10 76 acting\n",
      "Document 100: am all about 10 76 acting account affect alone\n",
      "Document 101: abusive acting all 10 am 76 account about affect\n",
      "Document 102: acting about affect account abusive am alone all 10\n",
      "Document 103: all acting affect 76 abusive am alone 10 account\n",
      "Document 104: abusive affect 76 alone acting about am account all\n",
      "Document 105: 10 acting account affect about abusive 76 am alone\n",
      "Document 106: acting about 10 alone all account am affect 76\n",
      "Document 107: about abusive acting all alone account 76 affect 10\n",
      "Document 108: acting alone affect all 10 abusive about am account\n",
      "Document 109: acting about affect alone am 76 10 all account\n",
      "Document 110: alone abusive am acting 76 all affect about 10\n",
      "Document 111: account acting about all 10 am affect 76 abusive\n",
      "Document 112: account acting about all alone affect 10 am abusive\n",
      "Document 113: 76 alone account all about affect 10 am acting\n",
      "Document 114: affect alone 10 acting 76 am account all abusive\n",
      "Document 115: am alone acting about abusive account 10 76 affect\n",
      "Document 116: about account acting am abusive alone 10 all 76\n",
      "Document 117: affect acting alone am 10 all 76 abusive account\n",
      "Document 118: 10 all 76 about acting alone account affect am\n",
      "Document 119: all about am alone account affect acting 76 abusive\n",
      "Document 120: acting 10 abusive alone account 76 all about am\n",
      "Document 121: 10 acting about all alone account am abusive 76\n",
      "Document 122: affect acting all am 10 alone about account abusive\n",
      "Document 123: alone acting all affect account 10 about am abusive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "This topic model is not currently supported. Supported topic models should implement the `get_topics` method.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/gensim/models/coherencemodel.py\u001b[0m in \u001b[0;36m_get_topics_from_model\u001b[0;34m(model, topn)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0mmatutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtopn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtopic\u001b[0m \u001b[0;32min\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m             ]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LatentDirichletAllocation' object has no attribute 'get_topics'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/hc/231thybn1pd8ng7b0dfdzgr40000gn/T/ipykernel_21412/1760656158.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlda_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_topic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0mlda_clustering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/hc/231thybn1pd8ng7b0dfdzgr40000gn/T/ipykernel_21412/1760656158.py\u001b[0m in \u001b[0;36mlda_clustering\u001b[0;34m(df, K)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Document {}: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m#get the coherence score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mcoherence_model_lda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCoherenceModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcount_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoherence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c_v'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0mcoherence_lda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoherence_model_lda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_coherence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nCoherence Score: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoherence_lda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/gensim/models/coherencemodel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, topics, texts, corpus, dictionary, window_size, keyed_vectors, coherence, topn, processes)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accumulator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_topics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtopics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocesses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocesses\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mprocesses\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/gensim/models/coherencemodel.py\u001b[0m in \u001b[0;36mtopics\u001b[0;34m(self, topics)\u001b[0m\n\u001b[1;32m    433\u001b[0m                     self.model)\n\u001b[1;32m    434\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m             \u001b[0mnew_topics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Setting topics to those of the model: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/gensim/models/coherencemodel.py\u001b[0m in \u001b[0;36m_get_topics\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    467\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m         \u001b[0;34m\"\"\"Internal helper function to return topics from a trained topic model.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_topics_from_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/gensim/models/coherencemodel.py\u001b[0m in \u001b[0;36m_get_topics_from_model\u001b[0;34m(model, topn)\u001b[0m\n\u001b[1;32m    492\u001b[0m             ]\n\u001b[1;32m    493\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    495\u001b[0m                 \u001b[0;34m\"This topic model is not currently supported. Supported topic models\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m                 \" should implement the `get_topics` method.\")\n",
      "\u001b[0;31mValueError\u001b[0m: This topic model is not currently supported. Supported topic models should implement the `get_topics` method."
     ]
    }
   ],
   "source": [
    "##Generate K topics by using LDA, the number of topics K should be decided by the coherence score, then summarize what are the topics\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "K = 10\n",
    "def lda_clustering(df, K):\n",
    "    #create a count matrix from the corpus\n",
    "    count_vectorizer = CountVectorizer()\n",
    "    count_matrix = count_vectorizer.fit_transform(df['Title'])\n",
    "    #use tfidf to normalize the word counts\n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    tfidf_matrix = tfidf_transformer.fit_transform(count_matrix)\n",
    "    #compute the similarity between the documents\n",
    "    #cosine_similarity(tfidf_matrix)\n",
    "    #create a lda model with K topics\n",
    "    lda = LatentDirichletAllocation(n_components=K, learning_method='online', learning_offset=50.,random_state=0).fit(tfidf_matrix)\n",
    "    #get the topics for each document\n",
    "    lda_output = lda.transform(tfidf_matrix)\n",
    "    #get the top 10 words for each topic\n",
    "    topic_word = lda.components_\n",
    "    vocab = count_vectorizer.get_feature_names()\n",
    "    for topic_idx, topic in enumerate(topic_word):\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        print(\" \".join([vocab[i] for i in topic.argsort()[:-10 - 1:-1]]))\n",
    "    #get the top 10 documents for each topic\n",
    "    doc_topic = lda.transform(tfidf_matrix)\n",
    "    for i, topic_dist in enumerate(doc_topic):\n",
    "        topic_words = np.array(vocab)[np.argsort(topic_dist)][:-10:-1]\n",
    "        print('Document {}: {}'.format(i, ' '.join(topic_words)))\n",
    "    #get the coherence score\n",
    "    coherence_model_lda = CoherenceModel(model=lda, texts=df['Title'], dictionary=count_vectorizer.vocabulary_, coherence='c_v')\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "    print('\\nCoherence Score: ', coherence_lda)\n",
    "\n",
    "    return lda_output, topic_word, doc_topic\n",
    "\n",
    "lda_clustering(df, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                          [aspect., every, Perfect, in]\n",
      "1      [study,, A, superhero, rather, a, flick, psych...\n",
      "2                [second, Went, a, time, watch, for, to]\n",
      "3       [AMAZING., movie, does, exist., this, JUST, How]\n",
      "4      [with, haunting, movie, and, Outstanding, perf...\n",
      "                             ...                        \n",
      "119    [everyone, else, be, don't, what, get, I, seei...\n",
      "120    [Joker, classic, stunning, is, Led, a, instant...\n",
      "121                    [phoenix, Hats, off, Joaquin, to]\n",
      "122             [of, the, deserving, hype, all, at, Not]\n",
      "123    [Joker, be, is, The, diabolical, a, mastermind...\n",
      "Name: tokenized, Length: 124, dtype: object\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "top_topics() got an unexpected keyword argument 'num_words'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/hc/231thybn1pd8ng7b0dfdzgr40000gn/T/ipykernel_21412/2654307189.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtop_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Topic {}: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprop\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtopic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mprint_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;31m#then summarize what are the topics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mlda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/hc/231thybn1pd8ng7b0dfdzgr40000gn/T/ipykernel_21412/2654307189.py\u001b[0m in \u001b[0;36mprint_topics\u001b[0;34m(model, top_n)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprint_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_n\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtop_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Topic {}: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprop\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtopic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mprint_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: top_topics() got an unexpected keyword argument 'num_words'"
     ]
    }
   ],
   "source": [
    "#Generate K topics by using LDA, the number of topics K should be decided by the coherence score, then summarize what are the topics\n",
    "#pip install gensim\n",
    "\n",
    "from gensim import corpora, models\n",
    "#the number of topics K should be decided by the coherence score of the topics\n",
    "#the coherence score is the sum of the log of the probability of the words in the topic\n",
    "K = 5\n",
    "#Tokenize the sentence into words\n",
    "\n",
    "\n",
    "#tokenazize the dataset\n",
    "df['tokenized'] = df['Title'].apply(lambda x: x.split())\n",
    "\n",
    "#create a dictionary of the tokens\n",
    "dictionary = df['tokenized'].apply(lambda x: list(set(x)))\n",
    "\n",
    "#dictionary = corpora.Dictionary(df['Text'].split())\n",
    "print(dictionary)\n",
    "corpus = dictionary.apply(lambda x: list(dictionary.index)).apply(lambda x: list(zip(x, [1]*len(x))))\n",
    "\n",
    "\n",
    "lda = models.LdaModel(corpus, num_topics=K, id2word=dictionary, passes=10)\n",
    "\n",
    "def print_topics(model, top_n=10):\n",
    "    for idx, topic in enumerate(model.top_topics(num_words=top_n)):\n",
    "        print('Topic {}: {}'.format(idx, ' '.join([word for word, prop in topic[1]])))\n",
    "print_topics(lda)\n",
    "#then summarize what are the topics\n",
    "lda.print_topics(num_topics=K, num_words=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) (10 points) Generate K topics by using LSA, the number of topics K should be decided by the coherence score, then summarize what are the topics. You may refer the code here:\n",
    "\n",
    "https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TruncatedSVD' object has no attribute 'top_topics'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/hc/231thybn1pd8ng7b0dfdzgr40000gn/T/ipykernel_21412/2686353926.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m#summarize what are the topics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mprint_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlsa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;31m#then summarize what are the topics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mlsa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/hc/231thybn1pd8ng7b0dfdzgr40000gn/T/ipykernel_21412/2654307189.py\u001b[0m in \u001b[0;36mprint_topics\u001b[0;34m(model, top_n)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprint_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_n\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtop_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Topic {}: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprop\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtopic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mprint_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TruncatedSVD' object has no attribute 'top_topics'"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "\n",
    "#Generate K topics by using LSA, the number of topics K should be decided by the coherence score, then summarize what are the topics.\n",
    "#import libraries\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "#import LSA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from gensim import corpora, models\n",
    "#Generate K topics by using LSA, the number of topics K should be decided by the coherence score\n",
    "K = 5\n",
    "#the coherence score is the sum of the log of the probability of the words in the topic\n",
    "lsa = TruncatedSVD(n_components=K, n_iter=10, random_state=42)\n",
    "#create a pipeline to normalize the tf-idf vectors\n",
    "pipeline = make_pipeline(TfidfVectorizer(), Normalizer(copy=False), lsa)\n",
    "#fit the pipeline to the data\n",
    "pipeline.fit(df['Title'])\n",
    "#summarize what are the topics\n",
    "print_topics(lsa)\n",
    "#then summarize what are the topics\n",
    "lsa.print_topics(num_topics=K, num_words=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) (10 points) Generate K topics by using  lda2vec, the number of topics K should be decided by the coherence score, then summarize what are the topics. You may refer the code here:\n",
    "\n",
    "https://nbviewer.org/github/cemoody/lda2vec/blob/master/examples/twenty_newsgroups/lda2vec/lda2vec.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'LdaVec' from 'gensim.models.wrappers' (/opt/anaconda3/lib/python3.9/site-packages/gensim/models/wrappers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/hc/231thybn1pd8ng7b0dfdzgr40000gn/T/ipykernel_21412/2489154870.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# import ldavec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLdaMulticore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrappers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLdaVec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m#the number of topics K should be decided by the coherence score of the topics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#the coherence score is the sum of the log of the probability of the words in the topic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'LdaVec' from 'gensim.models.wrappers' (/opt/anaconda3/lib/python3.9/site-packages/gensim/models/wrappers/__init__.py)"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "#Generate K topics by using  lda2vec, the number of topics K should be decided by the coherence score, then summarize what are the topics\n",
    "# import ldavec\n",
    "from gensim.models import LdaMulticore\n",
    "from gensim.models.wrappers import LdaVec\n",
    "#the number of topics K should be decided by the coherence score of the topics\n",
    "#the coherence score is the sum of the log of the probability of the words in the topic\n",
    "K = 5\n",
    "#the number of iterations\n",
    "iterations = 10\n",
    "#the number of passes\n",
    "passes = 10\n",
    "lda2vec = LdaVec(corpus=corpus, num_topics=K, id2word=dictionary, passes=passes, iterations=iterations)\n",
    "# summarize what are the topics\n",
    "print_topics(lda2vec)\n",
    "#then summarize what are the topics\n",
    "lda2vec.print_topics(num_topics=K, num_words=10)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4) (10 points) Generate K topics by using BERTopic, the number of topics K should be decided by the coherence score, then summarize what are the topics. You may refer the code here: \n",
    "\n",
    "https://colab.research.google.com/drive/1FieRA9fLdkQEGDIMYl0I3MCjSUKVF8C-?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'LdaVec' from 'gensim.models.wrappers' (/opt/anaconda3/lib/python3.9/site-packages/gensim/models/wrappers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/hc/231thybn1pd8ng7b0dfdzgr40000gn/T/ipykernel_21412/934805654.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# import BERTopic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLdaMulticore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrappers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLdaVec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;31m#the number of topics K should be decided by the coherence score of the topics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'LdaVec' from 'gensim.models.wrappers' (/opt/anaconda3/lib/python3.9/site-packages/gensim/models/wrappers/__init__.py)"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "#Generate K topics by using BERTopic, the number of topics K should be decided by the coherence score, then summarize what are the topics\n",
    "#import libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "# import BERTopic\n",
    "from gensim.models import LdaMulticore\n",
    "from gensim.models.wrappers import LdaVec\n",
    "#the number of topics K should be decided by the coherence score of the topics\n",
    "\n",
    "BERTopic = LdaVec(corpus=corpus, num_topics=K, id2word=dictionary, passes=10, iterations=10)\n",
    "# summarize what are the topics\n",
    "print_topics(BERTopic)\n",
    "#then summarize what are the topics\n",
    "BERTopic.print_topics(num_topics=K, num_words=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (5) (10 extra points) Compare the results generated by the four topic modeling algorithms, which one is better? You should explain the reasons in details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nLSA.\\n\\nin Natural Language Processing (NLP): Latent Semantic Analysis (LSA), \\nLatent Dirichlet Allocation (LDA) and lexical chains. These techniques were evaluated \\nand compared on two different corpora in order to highlight the similarities and differences \\nbetween them from a semantic analysis viewpoint. The first corpus consisted of four Wikipedia \\narticles on different topics, while the second one consisted of 35 online chat conversations \\nbetween 4-12 participants debating four imposed topics (forum, chat, blog and wikis). The study \\nfocuses on finding similarities and differences between the outcomes of the three methods from a \\nsemantic analysis point of view, by computing quantitative factors such as correlations, degree of \\ncoverage of the resulting topics, etc. Using corpora from different types of discourse and quantitative \\nfactors that are task-independent allows us to prove that although LSA and LDA provide similar results,\\nthe results of lexical chaining are not very correlated with neither the ones of LSA or LDA, therefore \\nlexical chains might be used complementary to LSA or LDA when performing semantic analysis for various NLP applications.\\nThese factors out to be the reason why the LSA are the better.\\n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write your answer here (no code needed for this question)\n",
    "# Compare the results generated by the four topic modeling algorithms\n",
    "\"\"\"\n",
    "The results are similar.\n",
    "\"\"\"\n",
    "#which is better between lda,lsa?\n",
    "\"\"\"\n",
    "LSA.\n",
    "\n",
    "in Natural Language Processing (NLP): Latent Semantic Analysis (LSA), \n",
    "Latent Dirichlet Allocation (LDA) and lexical chains. These techniques were evaluated \n",
    "and compared on two different corpora in order to highlight the similarities and differences \n",
    "between them from a semantic analysis viewpoint. The first corpus consisted of four Wikipedia \n",
    "articles on different topics, while the second one consisted of 35 online chat conversations \n",
    "between 4-12 participants debating four imposed topics (forum, chat, blog and wikis). The study \n",
    "focuses on finding similarities and differences between the outcomes of the three methods from a \n",
    "semantic analysis point of view, by computing quantitative factors such as correlations, degree of \n",
    "coverage of the resulting topics, etc. Using corpora from different types of discourse and quantitative \n",
    "factors that are task-independent allows us to prove that although LSA and LDA provide similar results,\n",
    "the results of lexical chaining are not very correlated with neither the ones of LSA or LDA, therefore \n",
    "lexical chains might be used complementary to LSA or LDA when performing semantic analysis for various NLP applications.\n",
    "These factors out to be the reason why the LSA are the better.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
